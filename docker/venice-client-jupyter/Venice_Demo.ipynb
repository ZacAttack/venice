{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b784a202-e278-4da1-8070-8f1b3aaf4bd5",
   "metadata": {},
   "source": [
    "![Alt text](venice-logo-lion.png \"Venice Logo\")\n",
    "\n",
    "# Welcome To Venice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52225a84-96f1-4093-9de7-435d6b7a3d43",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "This notebook is meant to serve as an interactive demo of Venice and some of it's concepts.  In this workbook we'll demo:\n",
    "\n",
    "* Preparing and Processing a dataset with Spark\n",
    "* Preparing a Venice store\n",
    "* Pushing the dataset to Venice and querying it\n",
    "\n",
    "To this end, we're going to demo a simple workflow that an AI engineer may go through when publishing processing and Publishing data to Venice.\n",
    "First, we'll start by getting a dataset from Hugging Face.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e686d-63bf-4d76-ba96-ceff46e9628b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing and Processing a dataset with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43106d-d33e-4617-8756-978e706d3df4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "For this demo to be interesting we'll need an interesting dataset, and the means to prepare it.  We'll use spark and a dataset from [Hugging Face](https://huggingface.co/)\n",
    "\n",
    "### What is Spark?\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is an open sourced data processing engine that is quite popular.  It enables users to set up data processing jobs in a distributed manner on a cluster (if you're familiar with Hadoop you probably understand some of the core concepts).  Venice supports writing data to a Venice store via Spark, and we will show case that capability here.  In the below cell, we'll use python and pyspark as our means to programmatically interact with a spark cluster that is running as part of this demo.\n",
    "\n",
    "### What is our dataset?\n",
    "\n",
    "We're going to grab a dataset from Hugging Face and upload it for processing to our spark cluster.  This dataset is a table of wine reviews!  Yay!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6729ffdd-580a-4851-916b-981da2126793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "|                name|              region| variety|rating|               notes|\n",
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "|1000 Stories Bour...|Mendocino, Califo...|Red Wine|  91.0|This is a very sp...|\n",
      "|1000 Stories Bour...|          California|Red Wine|  89.0|The California Go...|\n",
      "|1000 Stories Bour...|          California|Red Wine|  90.0|The California Go...|\n",
      "|1000 Stories Bour...|North Coast, Cali...|Red Wine|  91.0|The wine has a de...|\n",
      "|1000 Stories Bour...|          California|Red Wine|  90.0|Batch #004 is the...|\n",
      "|1000 Stories Bour...|          California|Red Wine|  91.0|1,000 Stories Bou...|\n",
      "|1000 Stories Bour...|          California|Red Wine|  92.0|Batch 55 embodies...|\n",
      "|12 Linajes Crianz...|Ribera del Duero,...|Red Wine|  92.0|Red with violet h...|\n",
      "|12 Linajes Reserv...|Ribera del Duero,...|Red Wine|  94.0|On the nose, a co...|\n",
      "|14 Hands Cabernet...|Columbia Valley, ...|Red Wine|  87.0|Concentrated arom...|\n",
      "+--------------------+--------------------+--------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "dataset = \"alfredodeza/wine-ratings\"\n",
    "\n",
    "# Initialize our Spark Session\n",
    "spark = SparkSession.builder.appName(\"VeniceWineReviewsDemo\").getOrCreate()\n",
    "\n",
    "# Query HuggingFace for the appropriate request paths for our dataset\n",
    "HUGGING_FACE_PARQUET_API = \"https://huggingface.co/api/datasets/{dataset}/parquet\"\n",
    "r = requests.get(HUGGING_FACE_PARQUET_API.format(dataset=dataset))\n",
    "train_parquet_files = r.json()['default']['train']\n",
    "\n",
    "# Download the datasets to our spark cluster\n",
    "for url in train_parquet_files:\n",
    "  spark.sparkContext.addFile(url)\n",
    "\n",
    "# Process the parquet files on the spark cluster so we can query them\n",
    "df = spark.read.parquet(SparkFiles.getRootDirectory() + \"/*.parquet\")\n",
    "\n",
    "# Displaying first 10 rows of our dataset\n",
    "df.show(n=10)\n",
    "\n",
    "# It's normally good practice to stop your spark session at the end, however, we're\n",
    "# going to reuse some of this state in the following cells, so we'll leave this commented\n",
    "# out for now.  If for your own purposes you want to try and play with other datasets,\n",
    "# be sure to run with spark.close between runs so as to clear out state and avoid errors.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7929a1-9e2a-48d9-85a3-23ca143153ae",
   "metadata": {},
   "source": [
    "### Using Spark To Prepare Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3344245-5a77-4ec6-83fa-3f9f301ecca4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "\n",
    "Now that we have our dataset, we need to manipulate it a bit. \n",
    "\n",
    "Venice is a key/value data store which uses [Avro](https://avro.apache.org/) as it's serialization format.  To that end, we'll need to process and transform our dataset to have a key/value structure and serialize it to Avro.\n",
    "\n",
    "The key to our dataset in Venice needs to be both non nullable and unique.  So we'll need to make sure those constraints are applied to our dataset before we can push it.  Looking at our dataset, we want to select a field for our key.  'name' seems a likely choice (and who wouldn't want a database where you can look up a wine rating by it's name!).  So we'll have spark process this data to form key/value pairs by grouping the columns in our dataset, remove null entries and duplicates, and finally serialize the dataset to avro.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03b03ff5-0c50-47d2-b507-fae24306c616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: struct (nullable = false)\n",
      " |    |-- region: string (nullable = true)\n",
      " |    |-- variety: string (nullable = true)\n",
      " |    |-- rating: float (nullable = true)\n",
      " |    |-- notes: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|1000 Stories Bour...|{California, Red ...|\n",
      "|1000 Stories Bour...|{North Coast, Cal...|\n",
      "|1000 Stories Bour...|{California, Red ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Create a struct type for the value column and group together the columns\n",
    "# which will make up our field value in the Venice store.\n",
    "df_transformed = df.withColumn(\"value\", struct(\n",
    "    col(\"region\").alias(\"region\"),\n",
    "    col(\"variety\").alias(\"variety\"),\n",
    "    col(\"rating\").alias(\"rating\"),\n",
    "    col(\"notes\").alias(\"notes\")\n",
    ")).select(\n",
    "    col(\"name\").alias(\"key\"),\n",
    "    col(\"value\")\n",
    ").dropDuplicates([\"key\"])\n",
    "\n",
    "# Define the new schema with key as non-nullable\n",
    "updated_schema = StructType([\n",
    "    StructField(\"key\", StringType(), nullable=False),\n",
    "    StructField(\"value\", StructType([\n",
    "        StructField(\"region\", StringType(), nullable=True),\n",
    "        StructField(\"variety\", StringType(), nullable=True),\n",
    "        StructField(\"rating\", FloatType(), nullable=True),\n",
    "        StructField(\"notes\", StringType(), nullable=True)\n",
    "    ]), nullable=False)\n",
    "])\n",
    "\n",
    "# Create new DataFrame with updated schema\n",
    "df_non_nullable_key = spark.createDataFrame(df_transformed.rdd, schema=updated_schema)\n",
    "\n",
    "# Show the resulting DataFrame schema\n",
    "df_non_nullable_key.printSchema()\n",
    "\n",
    "df_non_nullable_key.show(n=3)\n",
    "df_non_nullable_key.write.format(\"avro\").save(\"transformed_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba56d77-efa7-4173-a1f9-0bb63e09b3f3",
   "metadata": {},
   "source": [
    "## Preparing a Venice store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f7cc8-dc55-498b-baf9-9f8a02247ea5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "Looking good!  So we've got our data set.  Now we need to prepare a Venice store.  We've set up some endpoints in this demo that will enable you to interact with a venice cluster called \"venice-cluster0\".  We'll create a store called \"wine-ratings-store\" and we'll initialize the store with a schema that matches our dataset.  It's possible to generate the schema from the parqet file in the spark session, but to simplify this demo we've already generated the schema files.  We'll create our venice store with the below code:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c0e1bb2-9be4-493a-84a7-98ed0496a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[WARN] Running admin tool without SSL.\\n'\n",
      "b'WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\\n'\n",
      "b'2024-10-31 21:34:27 INFO [ControllerClient] [main] Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'{\\n'\n",
      "b'  \"error\" : \"Store wine-ratings-store already exists\"\\n'\n",
      "b'}\\n'\n",
      "b'Exception in thread \"main\" com.linkedin.venice.exceptions.VeniceException: Store wine-ratings-store already exists\\n'\n",
      "b'\\tat com.linkedin.venice.AdminTool.verifyStoreExistence(AdminTool.java:2347)\\n'\n",
      "b'\\tat com.linkedin.venice.AdminTool.createNewStore(AdminTool.java:828)\\n'\n",
      "b'\\tat com.linkedin.venice.AdminTool.main(AdminTool.java:292)\\n'\n",
      "b'[WARN] Running admin tool without SSL.\\n'\n",
      "b'2024-10-31 21:34:28 INFO [ControllerClient] [main] Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'{\"status\":\"success\"}\\n'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Utility function for printing command output\n",
    "def log_subprocess_output(pipe):\n",
    "    for line in iter(pipe.readline, b''):\n",
    "        print(line)\n",
    "\n",
    "# Arguments for Venice store creation\n",
    "create_store_args = [\n",
    "    \"./create-store.sh\",\n",
    "    \"http://venice-controller:5555\", \n",
    "    \"venice-cluster0\",\n",
    "    \"wine-ratings-store\", # Name of our store\n",
    "    \"~/wineKeySchema.avsc\",\n",
    "    \"~/wineValueSchema.avsc\"\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(create_store_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcdbfa-3a42-4e5d-983f-90cb673d7b1c",
   "metadata": {},
   "source": [
    "## Pushing the dataset to Venice and querying it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b012e-0fd4-4e49-aafa-34c4a519d5c3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "And now the moment we've been waiting for!  We've prepared our dataset and we've created our venice store.  Now let's transmit the data to Venice via spark!\n",
    "    \n",
    "With this demo we've provided the push job as a java jar which we'll submit to our spark cluster.  We've also prepared a properties file with the push job which contains some of the mandatory configurations needed for the push (things like the name of the store, the cluster's endpoint, and the path to the data that we'll need to transmit).  You can take a look at these configurations in batch-push-job.properties in the home directory of this demo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "96ff9b8b-7afd-4e1d-b460-55eb7ae0acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'24/10/31 21:49:01 INFO VenicePushJob: Constructing VenicePushJob: {\\n'\n",
      "b'\\tcontroller.request.retry.attempts: 5, \\n'\n",
      "b'\\tdata.writer.compute.job.class: com.linkedin.venice.hadoop.spark.datawriter.jobs.DataWriterSparkJob, \\n'\n",
      "b'\\tinput.path: /home/jovyan/transformed_avro, \\n'\n",
      "b'\\tpoll.job.status.interval.ms: 1000, \\n'\n",
      "b'\\tpush.job.status.upload.enable: false, \\n'\n",
      "b'\\tspark.native.input.format.enabled: true, \\n'\n",
      "b'\\tvenice.discover.urls: http://venice-controller:5555, \\n'\n",
      "b'\\tvenice.store.name: wine-ratings-store, \\n'\n",
      "b'\\tvenice.writer.close.timeout.ms: 500\\n'\n",
      "b'}\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Using /tmp/hadoop-jovyan/venice-push-job as shared temp directory\\n'\n",
      "b\"24/10/31 21:49:01 INFO VenicePushJob: Using /tmp/hadoop-jovyan/venice-push-job as this job's temp directory\\n\"\n",
      "b'24/10/31 21:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Going to use controller URL: http://venice-controller:5555  to discover cluster.\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Push job heartbeat is NOT enabled.\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: The store wine-ratings-store is discovered in Venice cluster venice-cluster0\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Running VenicePushJob: Venice Push Job\\n'\n",
      "b'  _    _           _                   \\n'\n",
      "b' | |  | |         | |                  \\n'\n",
      "b' | |__| | __ _  __| | ___   ___  _ __  \\n'\n",
      "b\" |  __  |/ _` |/ _` |/ _ \\\\ / _ \\\\| '_ \\\\ \\n\"\n",
      "b' | |  | | (_| | (_| | (_) | (_) | |_) |   \\n'\n",
      "b' |_|  |_|\\\\__,_|\\\\__,_|\\\\___/ \\\\___/| .__/\\n'\n",
      "b'                _______         | |     \\n'\n",
      "b'               |__   __|        |_|     \\n'\n",
      "b'                  | | ___               \\n'\n",
      "b'                  | |/ _ \\\\             \\n'\n",
      "b'     __      __   | | (_) |             \\n'\n",
      "b'     \\\\ \\\\    / /   |_|\\\\___/           \\n'\n",
      "b'      \\\\ \\\\  / /__ _ __  _  ___ ___     \\n'\n",
      "b\"       \\\\ \\\\/ / _ | '_ \\\\| |/ __/ _ \\\\  \\n\"\n",
      "b'        \\\\  |  __| | | | | (_|  __/     \\n'\n",
      "b'         \\\\/ \\\\___|_| |_|_|\\\\___\\\\___|  \\n'\n",
      "b'      ___        _     _                \\n'\n",
      "b'     |  _ \\\\     (_)   | |              \\n'\n",
      "b'     | |_) |_ __ _  __| | __ _  ___     \\n'\n",
      "b\"     |  _ <| '__| |/ _` |/ _` |/ _ \\\\   \\n\"\n",
      "b'     | |_) | |  | | (_| | (_| |  __/    \\n'\n",
      "b'     |____/|_|  |_|\\\\__,_|\\\\__, |\\\\___| \\n'\n",
      "b'                          __/ |         \\n'\n",
      "b'                         |___/          \\n'\n",
      "b'\\n'\n",
      "b'24/10/31 21:49:01 WARN VenicePushJob: Unable to send push job details for monitoring purpose. Feature is disabled\\n'\n",
      "b'24/10/31 21:49:01 INFO HadoopUtils: Trying to create path /tmp/hadoop-jovyan/venice-push-job with permission rwxrwxrwx\\n'\n",
      "b'24/10/31 21:49:01 INFO HadoopUtils: path /tmp/hadoop-jovyan/venice-push-job exists already\\n'\n",
      "b'24/10/31 21:49:01 INFO HadoopUtils: Trying to create path /tmp/hadoop-jovyan/venice-push-job/unknown_exec_id_1907587e4d29_64f3467a with permission rwx------\\n'\n",
      "b'24/10/31 21:49:01 INFO HadoopUtils: Creating path /tmp/hadoop-jovyan/venice-push-job/unknown_exec_id_1907587e4d29_64f3467a with permission rwx------\\n'\n",
      "b'24/10/31 21:49:01 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:01 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:01 WARN VenicePushJob: target region is not available for wine-ratings-store as it hybrid or deferred version swap enabled.\\n'\n",
      "b'24/10/31 21:49:01 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: No Compression dictionary will be generated with the compression strategy NO_OP and compressionMetricCollectionEnabled is disabled\\n'\n",
      "b'24/10/31 21:49:01 INFO VeniceVsonRecordReader: Path: part-00000-77bc3eb5-3303-40a5-bcfd-e14a46e6bac5-c000.avro is not a sequence file.\\n'\n",
      "b'24/10/31 21:49:01 INFO DefaultInputDataInfoProvider: Detected Avro input format.\\n'\n",
      "b'24/10/31 21:49:01 INFO AvroSerializer: Detected: AVRO_1_11 on the classpath.\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: No Compression dictionary will be generated with the compression strategy NO_OP and compressionMetricCollectionEnabled is disabled\\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Validating value schema: {\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\"fields\":[{\"name\":\"region\",\"type\":[\"string\",\"null\"]},{\"name\":\"variety\",\"type\":[\"string\",\"null\"]},{\"name\":\"rating\",\"type\":[\"float\",\"null\"]},{\"name\":\"notes\",\"type\":[\"string\",\"null\"]}]} for store: wine-ratings-store\\n'\n",
      "b'24/10/31 21:49:01 INFO FastSerdeCache: Generated classes dir: /tmp/generated16752540920458174648 and generation of generic FastSerializer is done for schema of type: topLevelRecord.value and fingerprint: 1319288651\\n'\n",
      "b'24/10/31 21:49:01 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'com/linkedin/avro/fastserde/generated/serialization/AVRO_1_11/value_GenericSerializer_1319288651.java\\n'\n",
      "b'24/10/31 21:49:01 INFO FastSerdeBase: Starting compilation for the generated source file: /tmp/generated16752540920458174648/com/linkedin/avro/fastserde/generated/serialization/AVRO_1_11/value_GenericSerializer_1319288651.java \\n'\n",
      "b'24/10/31 21:49:01 INFO VenicePushJob: Got schema id: 1 for value schema: {\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\"fields\":[{\"name\":\"region\",\"type\":[\"string\",\"null\"]},{\"name\":\"variety\",\"type\":[\"string\",\"null\"]},{\"name\":\"rating\",\"type\":[\"float\",\"null\"]},{\"name\":\"notes\",\"type\":[\"string\",\"null\"]}]} of store: wine-ratings-store\\n'\n",
      "b'24/10/31 21:49:01 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:02 INFO FastSerdeBase: Successfully compiled class value_GenericSerializer_1319288651 defined at source file: /tmp/generated16752540920458174648/com/linkedin/avro/fastserde/generated/serialization/AVRO_1_11/value_GenericSerializer_1319288651.java\\n'\n",
      "b'24/10/31 21:49:03 INFO VenicePushJob: VersionCreationResponse(partitions: 3, replicas: 1, kafkaTopic: wine-ratings-store_v5, kafkaBootstrapServers: kafka:9092, kafkaSourceRegion: , enableSSL: false, compressionStrategy: NO_OP, partitionerClass: com.linkedin.venice.partitioner.DefaultVenicePartitioner, partitionerParams: {}, amplificationFactor: 1, daVinciPushStatusStoreEnabled: false, super: VersionResponse(version: 5, super: ControllerResponse(cluster: venice-cluster0, name: wine-ratings-store, error: null, errorType: null, exceptionType: null)))\\n'\n",
      "b'24/10/31 21:49:03 INFO VenicePushJob: Job ID: Venice Push Job\\n'\n",
      "b'Kafka URL: kafka:9092\\n'\n",
      "b'Kafka Topic: wine-ratings-store_v5\\n'\n",
      "b'Kafka topic partition count: 3\\n'\n",
      "b'Kafka Queue Bytes: 1000000\\n'\n",
      "b'Input Directory: /home/jovyan/transformed_avro\\n'\n",
      "b'Venice Store Name: wine-ratings-store\\n'\n",
      "b'Venice Cluster Name: venice-cluster0\\n'\n",
      "b'Venice URL: http://venice-controller:5555\\n'\n",
      "b'File Schema: {\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":[{\"name\":\"key\",\"type\":\"string\"},{\"name\":\"value\",\"type\":{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\"fields\":[{\"name\":\"region\",\"type\":[\"string\",\"null\"]},{\"name\":\"variety\",\"type\":[\"string\",\"null\"]},{\"name\":\"rating\",\"type\":[\"float\",\"null\"]},{\"name\":\"notes\",\"type\":[\"string\",\"null\"]}]}}]}\\n'\n",
      "b'Avro key schema: \"string\"\\n'\n",
      "b'Avro value schema: {\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\"fields\":[{\"name\":\"region\",\"type\":[\"string\",\"null\"]},{\"name\":\"variety\",\"type\":[\"string\",\"null\"]},{\"name\":\"rating\",\"type\":[\"float\",\"null\"]},{\"name\":\"notes\",\"type\":[\"string\",\"null\"]}]}\\n'\n",
      "b'Total input data file size: 5.579724311828613 MB. This could be the size of compressed data if the underlying filesystem compresses it\\n'\n",
      "b'Max Venice Record Size: -1\\n'\n",
      "b'Is Chunking Enabled: false\\n'\n",
      "b'Is Replication Metadata Chunking Enabled: false\\n'\n",
      "b'Is incremental push: false\\n'\n",
      "b'Is duplicated key allowed: false\\n'\n",
      "b'Is source ETL data: false\\n'\n",
      "b'ETL value schema transformation : NONE\\n'\n",
      "b'Is Kafka Input Format: false\\n'\n",
      "b'24/10/31 21:49:03 INFO VenicePushJob: Configuring data writer job\\n'\n",
      "b\"24/10/31 21:49:03 INFO VenicePushJob: Using 'class com.linkedin.venice.hadoop.spark.datawriter.jobs.DataWriterSparkJob' for data writer job\\n\"\n",
      "b'24/10/31 21:49:03 INFO SparkContext: Running Spark version 3.3.0\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceUtils: ==============================================================\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceUtils: No custom resources configured for spark.driver.\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceUtils: ==============================================================\\n'\n",
      "b'24/10/31 21:49:03 INFO SparkContext: Submitted application: com.linkedin.venice.hadoop.VenicePushJob\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceProfile: Limiting resource is cpu\\n'\n",
      "b'24/10/31 21:49:03 INFO ResourceProfileManager: Added ResourceProfile id: 0\\n'\n",
      "b'24/10/31 21:49:03 INFO SecurityManager: Changing view acls to: jovyan\\n'\n",
      "b'24/10/31 21:49:03 INFO SecurityManager: Changing modify acls to: jovyan\\n'\n",
      "b'24/10/31 21:49:03 INFO SecurityManager: Changing view acls groups to: \\n'\n",
      "b'24/10/31 21:49:03 INFO SecurityManager: Changing modify acls groups to: \\n'\n",
      "b'24/10/31 21:49:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\\n'\n",
      "b\"24/10/31 21:49:03 INFO Utils: Successfully started service 'sparkDriver' on port 35531.\\n\"\n",
      "b'24/10/31 21:49:03 INFO SparkEnv: Registering MapOutputTracker\\n'\n",
      "b'24/10/31 21:49:03 INFO SparkEnv: Registering BlockManagerMaster\\n'\n",
      "b'24/10/31 21:49:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\\n'\n",
      "b'24/10/31 21:49:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\\n'\n",
      "b'24/10/31 21:49:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\\n'\n",
      "b'24/10/31 21:49:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-514d038b-129c-467e-ae75-9efa2c1d6daa\\n'\n",
      "b'24/10/31 21:49:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\\n'\n",
      "b'24/10/31 21:49:03 INFO SparkEnv: Registering OutputCommitCoordinator\\n'\n",
      "b\"24/10/31 21:49:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\\n\"\n",
      "b\"24/10/31 21:49:03 INFO Utils: Successfully started service 'SparkUI' on port 4041.\\n\"\n",
      "b'24/10/31 21:49:03 INFO SparkContext: Added JAR file:/home/jovyan/bin/venice-push-job-all.jar at spark://venice-client-jupyter:35531/jars/venice-push-job-all.jar with timestamp 1730411343358\\n'\n",
      "b'24/10/31 21:49:03 INFO Executor: Starting executor ID driver on host venice-client-jupyter\\n'\n",
      "b\"24/10/31 21:49:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\\n\"\n",
      "b'24/10/31 21:49:03 INFO Executor: Fetching spark://venice-client-jupyter:35531/jars/venice-push-job-all.jar with timestamp 1730411343358\\n'\n",
      "b'24/10/31 21:49:03 INFO TransportClientFactory: Successfully created connection to venice-client-jupyter/172.19.0.8:35531 after 12 ms (0 ms spent in bootstraps)\\n'\n",
      "b'24/10/31 21:49:03 INFO Utils: Fetching spark://venice-client-jupyter:35531/jars/venice-push-job-all.jar to /tmp/spark-5a9c131a-1476-40b0-a87e-ff10fc8afca6/userFiles-3be8598b-4b4a-4c88-bef2-970893c904ba/fetchFileTemp9631678661206838076.tmp\\n'\n",
      "b'24/10/31 21:49:04 INFO Executor: Adding file:/tmp/spark-5a9c131a-1476-40b0-a87e-ff10fc8afca6/userFiles-3be8598b-4b4a-4c88-bef2-970893c904ba/venice-push-job-all.jar to class loader\\n'\n",
      "b\"24/10/31 21:49:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44979.\\n\"\n",
      "b'24/10/31 21:49:04 INFO NettyBlockTransferService: Server created on venice-client-jupyter:44979\\n'\n",
      "b'24/10/31 21:49:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\\n'\n",
      "b'24/10/31 21:49:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, venice-client-jupyter, 44979, None)\\n'\n",
      "b'24/10/31 21:49:04 INFO BlockManagerMasterEndpoint: Registering block manager venice-client-jupyter:44979 with 434.4 MiB RAM, BlockManagerId(driver, venice-client-jupyter, 44979, None)\\n'\n",
      "b'24/10/31 21:49:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, venice-client-jupyter, 44979, None)\\n'\n",
      "b'24/10/31 21:49:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, venice-client-jupyter, 44979, None)\\n'\n",
      "b\"24/10/31 21:49:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\\n\"\n",
      "b\"24/10/31 21:49:04 INFO SharedState: Warehouse path is 'file:/home/jovyan/spark-warehouse'.\\n\"\n",
      "b'24/10/31 21:49:05 INFO InMemoryFileIndex: It took 21 ms to list leaf files for 1 paths.\\n'\n",
      "b'24/10/31 21:49:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.8 KiB, free 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1605.0 B, free 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on venice-client-jupyter:44979 (size: 1605.0 B, free: 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO SparkContext: Created broadcast 0 from Venice Push Job:venice_push_job-wine-ratings-store_v5\\n'\n",
      "b'24/10/31 21:49:05 INFO FileSourceStrategy: Pushed Filters: \\n'\n",
      "b'24/10/31 21:49:05 INFO FileSourceStrategy: Post-Scan Filters: \\n'\n",
      "b'24/10/31 21:49:05 INFO FileSourceStrategy: Output Data Schema: struct<key: string, value: struct<region: string, variety: string, rating: float, notes: string ... 2 more fields>>\\n'\n",
      "b'24/10/31 21:49:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 203.2 KiB, free 434.2 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 434.2 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on venice-client-jupyter:44979 (size: 34.9 KiB, free: 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:05 INFO SparkContext: Created broadcast 1 from Venice Push Job:venice_push_job-wine-ratings-store_v5\\n'\n",
      "b'24/10/31 21:49:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\\n'\n",
      "b'24/10/31 21:49:05 INFO VenicePushJob: Triggering data writer job\\n'\n",
      "b'24/10/31 21:49:05 INFO AbstractDataWriterSparkJob: Triggering Spark job for data writer\\n'\n",
      "b'24/10/31 21:49:06 INFO CodeGenerator: Code generated in 66.019375 ms\\n'\n",
      "b'24/10/31 21:49:06 INFO CodeGenerator: Code generated in 5.440334 ms\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Registering RDD 7 (Venice Push Job:venice_push_job-wine-ratings-store_v5) as input to shuffle 1\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Registering RDD 15 (Venice Push Job:venice_push_job-wine-ratings-store_v5) as input to shuffle 0\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Got map stage job 0 (Venice Push Job:venice_push_job-wine-ratings-store_v5) with 3 output partitions\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (Venice Push Job:venice_push_job-wine-ratings-store_v5)\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[7] at Venice Push Job:venice_push_job-wine-ratings-store_v5), which has no missing parents\\n'\n",
      "b'24/10/31 21:49:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 35.0 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on venice-client-jupyter:44979 (size: 16.5 KiB, free: 434.3 MiB)\\n'\n",
      "b'24/10/31 21:49:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\\n'\n",
      "b'24/10/31 21:49:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[7] at Venice Push Job:venice_push_job-wine-ratings-store_v5) (first 15 tasks are for partitions Vector(0, 1))\\n'\n",
      "b'24/10/31 21:49:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\\n'\n",
      "b'24/10/31 21:49:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (venice-client-jupyter, executor driver, partition 0, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (venice-client-jupyter, executor driver, partition 1, PROCESS_LOCAL, 4954 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\\n'\n",
      "b'24/10/31 21:49:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\\n'\n",
      "b'24/10/31 21:49:06 INFO CodeGenerator: Code generated in 292.526042 ms\\n'\n",
      "b'24/10/31 21:49:06 INFO FileScanRDD: Reading File path: file:///home/jovyan/transformed_avro/part-00001-77bc3eb5-3303-40a5-bcfd-e14a46e6bac5-c000.avro, range: 0-2930728, partition values: [empty row]\\n'\n",
      "b'24/10/31 21:49:06 INFO FileScanRDD: Reading File path: file:///home/jovyan/transformed_avro/part-00000-77bc3eb5-3303-40a5-bcfd-e14a46e6bac5-c000.avro, range: 0-2920037, partition values: [empty row]\\n'\n",
      "b'24/10/31 21:49:06 INFO CodeGenerator: Code generated in 5.891792 ms\\n'\n",
      "b'24/10/31 21:49:06 INFO AbstractInputRecordProcessor: Task ID 0 successfully sprayed all partitions, to ensure that all Reducers come up.\\n'\n",
      "b'24/10/31 21:49:07 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2707 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2707 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 874 ms on venice-client-jupyter (executor driver) (1/2)\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 869 ms on venice-client-jupyter (executor driver) (2/2)\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: ShuffleMapStage 0 (Venice Push Job:venice_push_job-wine-ratings-store_v5) finished in 0.985 s\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: looking for newly runnable stages\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: running: Set()\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1)\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: failed: Set()\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[15] at Venice Push Job:venice_push_job-wine-ratings-store_v5), which has no missing parents\\n'\n",
      "b'24/10/31 21:49:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 31.7 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.1 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on venice-client-jupyter:44979 (size: 14.1 KiB, free: 434.3 MiB)\\n'\n",
      "b'24/10/31 21:49:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\\n'\n",
      "b'24/10/31 21:49:07 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[15] at Venice Push Job:venice_push_job-wine-ratings-store_v5) (first 15 tasks are for partitions Vector(0, 1, 2))\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks resource profile 0\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (venice-client-jupyter, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (venice-client-jupyter, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (venice-client-jupyter, executor driver, partition 2, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\\n'\n",
      "b'24/10/31 21:49:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\\n'\n",
      "b'24/10/31 21:49:07 INFO Executor: Running task 2.0 in stage 1.0 (TID 4)\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Getting 2 (2.6 MiB) non-empty blocks including 2 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Getting 2 (2.6 MiB) non-empty blocks including 2 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Getting 2 (2.6 MiB) non-empty blocks including 2 (2.6 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\\n'\n",
      "b'24/10/31 21:49:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\\n'\n",
      "b'24/10/31 21:49:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on venice-client-jupyter:44979 in memory (size: 16.5 KiB, free: 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:07 INFO CodeGenerator: Code generated in 3.932709 ms\\n'\n",
      "b'24/10/31 21:49:07 INFO CodeGenerator: Code generated in 10.138667 ms\\n'\n",
      "b'24/10/31 21:49:07 INFO ApacheKafkaProducerConfig: Will initialize a non-SSL Kafka producer\\n'\n",
      "b'24/10/31 21:49:07 INFO ApacheKafkaProducerConfig: Will initialize a non-SSL Kafka producer\\n'\n",
      "b'24/10/31 21:49:07 INFO ApacheKafkaProducerConfig: Will initialize a non-SSL Kafka producer\\n'\n",
      "b'24/10/31 21:49:07 INFO ProducerConfig: ProducerConfig values: \\n'\n",
      "b'\\tacks = all\\n'\n",
      "b'\\tallow.auto.create.topics = false\\n'\n",
      "b'\\tbatch.size = 16384\\n'\n",
      "b'\\tbootstrap.servers = [kafka:9092]\\n'\n",
      "b'\\tbuffer.memory = 33554432\\n'\n",
      "b'\\tclient.dns.lookup = use_all_dns_ips\\n'\n",
      "b'\\tclient.id = wine-ratings-store_v5\\n'\n",
      "b'\\tcompression.type = gzip\\n'\n",
      "b'\\tconnections.max.idle.ms = 540000\\n'\n",
      "b'\\tdelivery.timeout.ms = 2147483647\\n'\n",
      "b'\\tenable.idempotence = false\\n'\n",
      "b'\\tinterceptor.classes = []\\n'\n",
      "b'\\tkey.serializer = class com.linkedin.venice.serialization.KafkaKeySerializer\\n'\n",
      "b'\\tli.client.cluster.metadata.expire.time.ms = 3600000\\n'\n",
      "b'\\tli.client.software.name.and.commit = li-oss-producer-java\\n'\n",
      "b'\\tli.update.metadata.last.refresh.time.upon.node.disconnect = true\\n'\n",
      "b'\\tlinger.ms = 0\\n'\n",
      "b'\\tlinkedin.least.loaded.node.algorithm = VANILLA\\n'\n",
      "b'\\tmax.block.ms = 9223372036854775807\\n'\n",
      "b'\\tmax.in.flight.requests.per.connection = 1\\n'\n",
      "b'\\tmax.request.size = 1048576\\n'\n",
      "b'\\tmetadata.max.age.ms = 300000\\n'\n",
      "b'\\tmetadata.topic.expiry.ms = 300000\\n'\n",
      "b'\\tmetric.reporters = []\\n'\n",
      "b'\\tmetrics.num.samples = 2\\n'\n",
      "b'\\tmetrics.recording.level = INFO\\n'\n",
      "b'\\tmetrics.replace.on.duplicate = false\\n'\n",
      "b'\\tmetrics.sample.window.ms = 30000\\n'\n",
      "b'\\tpartitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\\n'\n",
      "b'\\treceive.buffer.bytes = 32768\\n'\n",
      "b'\\treconnect.backoff.max.ms = 1000\\n'\n",
      "b'\\treconnect.backoff.ms = 50\\n'\n",
      "b'\\trequest.timeout.ms = 2147483647\\n'\n",
      "b'\\tretries = 2147483647\\n'\n",
      "b'\\tretry.backoff.ms = 1000\\n'\n",
      "b'\\tsasl.client.callback.handler.class = null\\n'\n",
      "b'\\tsasl.jaas.config = null\\n'\n",
      "b'\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n'\n",
      "b'\\tsasl.kerberos.min.time.before.relogin = 60000\\n'\n",
      "b'\\tsasl.kerberos.service.name = null\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.callback.handler.class = null\\n'\n",
      "b'\\tsasl.login.class = null\\n'\n",
      "b'\\tsasl.login.refresh.buffer.seconds = 300\\n'\n",
      "b'\\tsasl.login.refresh.min.period.seconds = 60\\n'\n",
      "b'\\tsasl.login.refresh.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.refresh.window.jitter = 0.05\\n'\n",
      "b'\\tsasl.mechanism = GSSAPI\\n'\n",
      "b'\\tsecurity.protocol = PLAINTEXT\\n'\n",
      "b'\\tsecurity.providers = null\\n'\n",
      "b'\\tsend.buffer.bytes = 131072\\n'\n",
      "b'\\tssl.cipher.suites = null\\n'\n",
      "b'\\tssl.context.provider.class = org.apache.kafka.common.security.ssl.SimpleSslContextProvider\\n'\n",
      "b'\\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\\n'\n",
      "b'\\tssl.endpoint.identification.algorithm = https\\n'\n",
      "b'\\tssl.key.password = null\\n'\n",
      "b'\\tssl.keymanager.algorithm = SunX509\\n'\n",
      "b'\\tssl.keystore.location = null\\n'\n",
      "b'\\tssl.keystore.password = null\\n'\n",
      "b'\\tssl.keystore.type = JKS\\n'\n",
      "b'\\tssl.protocol = TLS\\n'\n",
      "b'\\tssl.provider = null\\n'\n",
      "b'\\tssl.secure.random.implementation = null\\n'\n",
      "b'\\tssl.trustmanager.algorithm = PKIX\\n'\n",
      "b'\\tssl.truststore.location = null\\n'\n",
      "b'\\tssl.truststore.password = null\\n'\n",
      "b'\\tssl.truststore.type = JKS\\n'\n",
      "b'\\ttransaction.timeout.ms = 60000\\n'\n",
      "b'\\ttransactional.id = null\\n'\n",
      "b'\\tvalue.serializer = class com.linkedin.venice.serialization.avro.KafkaValueSerializer\\n'\n",
      "b'\\n'\n",
      "b'24/10/31 21:49:07 INFO ProducerConfig: ProducerConfig values: \\n'\n",
      "b'\\tacks = all\\n'\n",
      "b'\\tallow.auto.create.topics = false\\n'\n",
      "b'\\tbatch.size = 16384\\n'\n",
      "b'\\tbootstrap.servers = [kafka:9092]\\n'\n",
      "b'\\tbuffer.memory = 33554432\\n'\n",
      "b'\\tclient.dns.lookup = use_all_dns_ips\\n'\n",
      "b'\\tclient.id = wine-ratings-store_v5\\n'\n",
      "b'\\tcompression.type = gzip\\n'\n",
      "b'\\tconnections.max.idle.ms = 540000\\n'\n",
      "b'\\tdelivery.timeout.ms = 2147483647\\n'\n",
      "b'\\tenable.idempotence = false\\n'\n",
      "b'\\tinterceptor.classes = []\\n'\n",
      "b'\\tkey.serializer = class com.linkedin.venice.serialization.KafkaKeySerializer\\n'\n",
      "b'\\tli.client.cluster.metadata.expire.time.ms = 3600000\\n'\n",
      "b'\\tli.client.software.name.and.commit = li-oss-producer-java\\n'\n",
      "b'\\tli.update.metadata.last.refresh.time.upon.node.disconnect = true\\n'\n",
      "b'\\tlinger.ms = 0\\n'\n",
      "b'\\tlinkedin.least.loaded.node.algorithm = VANILLA\\n'\n",
      "b'\\tmax.block.ms = 9223372036854775807\\n'\n",
      "b'\\tmax.in.flight.requests.per.connection = 1\\n'\n",
      "b'\\tmax.request.size = 1048576\\n'\n",
      "b'\\tmetadata.max.age.ms = 300000\\n'\n",
      "b'\\tmetadata.topic.expiry.ms = 300000\\n'\n",
      "b'\\tmetric.reporters = []\\n'\n",
      "b'\\tmetrics.num.samples = 2\\n'\n",
      "b'\\tmetrics.recording.level = INFO\\n'\n",
      "b'\\tmetrics.replace.on.duplicate = false\\n'\n",
      "b'\\tmetrics.sample.window.ms = 30000\\n'\n",
      "b'\\tpartitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\\n'\n",
      "b'\\treceive.buffer.bytes = 32768\\n'\n",
      "b'\\treconnect.backoff.max.ms = 1000\\n'\n",
      "b'\\treconnect.backoff.ms = 50\\n'\n",
      "b'\\trequest.timeout.ms = 2147483647\\n'\n",
      "b'\\tretries = 2147483647\\n'\n",
      "b'\\tretry.backoff.ms = 1000\\n'\n",
      "b'\\tsasl.client.callback.handler.class = null\\n'\n",
      "b'\\tsasl.jaas.config = null\\n'\n",
      "b'\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n'\n",
      "b'\\tsasl.kerberos.min.time.before.relogin = 60000\\n'\n",
      "b'\\tsasl.kerberos.service.name = null\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.callback.handler.class = null\\n'\n",
      "b'\\tsasl.login.class = null\\n'\n",
      "b'\\tsasl.login.refresh.buffer.seconds = 300\\n'\n",
      "b'\\tsasl.login.refresh.min.period.seconds = 60\\n'\n",
      "b'\\tsasl.login.refresh.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.refresh.window.jitter = 0.05\\n'\n",
      "b'\\tsasl.mechanism = GSSAPI\\n'\n",
      "b'\\tsecurity.protocol = PLAINTEXT\\n'\n",
      "b'\\tsecurity.providers = null\\n'\n",
      "b'\\tsend.buffer.bytes = 131072\\n'\n",
      "b'\\tssl.cipher.suites = null\\n'\n",
      "b'\\tssl.context.provider.class = org.apache.kafka.common.security.ssl.SimpleSslContextProvider\\n'\n",
      "b'\\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\\n'\n",
      "b'\\tssl.endpoint.identification.algorithm = https\\n'\n",
      "b'\\tssl.key.password = null\\n'\n",
      "b'\\tssl.keymanager.algorithm = SunX509\\n'\n",
      "b'\\tssl.keystore.location = null\\n'\n",
      "b'\\tssl.keystore.password = null\\n'\n",
      "b'\\tssl.keystore.type = JKS\\n'\n",
      "b'\\tssl.protocol = TLS\\n'\n",
      "b'\\tssl.provider = null\\n'\n",
      "b'\\tssl.secure.random.implementation = null\\n'\n",
      "b'\\tssl.trustmanager.algorithm = PKIX\\n'\n",
      "b'\\tssl.truststore.location = null\\n'\n",
      "b'\\tssl.truststore.password = null\\n'\n",
      "b'\\tssl.truststore.type = JKS\\n'\n",
      "b'\\ttransaction.timeout.ms = 60000\\n'\n",
      "b'\\ttransactional.id = null\\n'\n",
      "b'\\tvalue.serializer = class com.linkedin.venice.serialization.avro.KafkaValueSerializer\\n'\n",
      "b'\\n'\n",
      "b'24/10/31 21:49:07 INFO ProducerConfig: ProducerConfig values: \\n'\n",
      "b'\\tacks = all\\n'\n",
      "b'\\tallow.auto.create.topics = false\\n'\n",
      "b'\\tbatch.size = 16384\\n'\n",
      "b'\\tbootstrap.servers = [kafka:9092]\\n'\n",
      "b'\\tbuffer.memory = 33554432\\n'\n",
      "b'\\tclient.dns.lookup = use_all_dns_ips\\n'\n",
      "b'\\tclient.id = wine-ratings-store_v5\\n'\n",
      "b'\\tcompression.type = gzip\\n'\n",
      "b'\\tconnections.max.idle.ms = 540000\\n'\n",
      "b'\\tdelivery.timeout.ms = 2147483647\\n'\n",
      "b'\\tenable.idempotence = false\\n'\n",
      "b'\\tinterceptor.classes = []\\n'\n",
      "b'\\tkey.serializer = class com.linkedin.venice.serialization.KafkaKeySerializer\\n'\n",
      "b'\\tli.client.cluster.metadata.expire.time.ms = 3600000\\n'\n",
      "b'\\tli.client.software.name.and.commit = li-oss-producer-java\\n'\n",
      "b'\\tli.update.metadata.last.refresh.time.upon.node.disconnect = true\\n'\n",
      "b'\\tlinger.ms = 0\\n'\n",
      "b'\\tlinkedin.least.loaded.node.algorithm = VANILLA\\n'\n",
      "b'\\tmax.block.ms = 9223372036854775807\\n'\n",
      "b'\\tmax.in.flight.requests.per.connection = 1\\n'\n",
      "b'\\tmax.request.size = 1048576\\n'\n",
      "b'\\tmetadata.max.age.ms = 300000\\n'\n",
      "b'\\tmetadata.topic.expiry.ms = 300000\\n'\n",
      "b'\\tmetric.reporters = []\\n'\n",
      "b'\\tmetrics.num.samples = 2\\n'\n",
      "b'\\tmetrics.recording.level = INFO\\n'\n",
      "b'\\tmetrics.replace.on.duplicate = false\\n'\n",
      "b'\\tmetrics.sample.window.ms = 30000\\n'\n",
      "b'\\tpartitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner\\n'\n",
      "b'\\treceive.buffer.bytes = 32768\\n'\n",
      "b'\\treconnect.backoff.max.ms = 1000\\n'\n",
      "b'\\treconnect.backoff.ms = 50\\n'\n",
      "b'\\trequest.timeout.ms = 2147483647\\n'\n",
      "b'\\tretries = 2147483647\\n'\n",
      "b'\\tretry.backoff.ms = 1000\\n'\n",
      "b'\\tsasl.client.callback.handler.class = null\\n'\n",
      "b'\\tsasl.jaas.config = null\\n'\n",
      "b'\\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\\n'\n",
      "b'\\tsasl.kerberos.min.time.before.relogin = 60000\\n'\n",
      "b'\\tsasl.kerberos.service.name = null\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.jitter = 0.05\\n'\n",
      "b'\\tsasl.kerberos.ticket.renew.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.callback.handler.class = null\\n'\n",
      "b'\\tsasl.login.class = null\\n'\n",
      "b'\\tsasl.login.refresh.buffer.seconds = 300\\n'\n",
      "b'\\tsasl.login.refresh.min.period.seconds = 60\\n'\n",
      "b'\\tsasl.login.refresh.window.factor = 0.8\\n'\n",
      "b'\\tsasl.login.refresh.window.jitter = 0.05\\n'\n",
      "b'\\tsasl.mechanism = GSSAPI\\n'\n",
      "b'\\tsecurity.protocol = PLAINTEXT\\n'\n",
      "b'\\tsecurity.providers = null\\n'\n",
      "b'\\tsend.buffer.bytes = 131072\\n'\n",
      "b'\\tssl.cipher.suites = null\\n'\n",
      "b'\\tssl.context.provider.class = org.apache.kafka.common.security.ssl.SimpleSslContextProvider\\n'\n",
      "b'\\tssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]\\n'\n",
      "b'\\tssl.endpoint.identification.algorithm = https\\n'\n",
      "b'\\tssl.key.password = null\\n'\n",
      "b'\\tssl.keymanager.algorithm = SunX509\\n'\n",
      "b'\\tssl.keystore.location = null\\n'\n",
      "b'\\tssl.keystore.password = null\\n'\n",
      "b'\\tssl.keystore.type = JKS\\n'\n",
      "b'\\tssl.protocol = TLS\\n'\n",
      "b'\\tssl.provider = null\\n'\n",
      "b'\\tssl.secure.random.implementation = null\\n'\n",
      "b'\\tssl.trustmanager.algorithm = PKIX\\n'\n",
      "b'\\tssl.truststore.location = null\\n'\n",
      "b'\\tssl.truststore.password = null\\n'\n",
      "b'\\tssl.truststore.type = JKS\\n'\n",
      "b'\\ttransaction.timeout.ms = 60000\\n'\n",
      "b'\\ttransactional.id = null\\n'\n",
      "b'\\tvalue.serializer = class com.linkedin.venice.serialization.avro.KafkaValueSerializer\\n'\n",
      "b'\\n'\n",
      "b\"24/10/31 21:49:07 INFO InternalAvroSpecificSerializer: Serializer doesn't have schemaReader\\n\"\n",
      "b\"24/10/31 21:49:07 INFO InternalAvroSpecificSerializer: Serializer doesn't have schemaReader\\n\"\n",
      "b\"24/10/31 21:49:07 INFO InternalAvroSpecificSerializer: Serializer doesn't have schemaReader\\n\"\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka version: 2.4.1.65\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka commitId: 4b7ef9d07758b292\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka startTimeMs: 1730411347829\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka version: 2.4.1.65\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka commitId: 4b7ef9d07758b292\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka startTimeMs: 1730411347829\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka version: 2.4.1.65\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka commitId: 4b7ef9d07758b292\\n'\n",
      "b'24/10/31 21:49:07 INFO AppInfoParser: Kafka startTimeMs: 1730411347829\\n'\n",
      "b'24/10/31 21:49:07 INFO Metadata: [Producer clientId=wine-ratings-store_v5] Cluster ID: nPMXg8SAQT6byUBhsSym-Q\\n'\n",
      "b'24/10/31 21:49:07 INFO Metadata: [Producer clientId=wine-ratings-store_v5] Cluster ID: nPMXg8SAQT6byUBhsSym-Q\\n'\n",
      "b'24/10/31 21:49:07 INFO Metadata: [Producer clientId=wine-ratings-store_v5] Cluster ID: nPMXg8SAQT6byUBhsSym-Q\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: DataWriterComputeJob records processed: 10000, total time spent: 51ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka records produced: 9988, total time spent: 556ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: DataWriterComputeJob records processed: 10000, total time spent: 56ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka records produced: 9936, total time spent: 554ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: DataWriterComputeJob records processed: 10000, total time spent: 54ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka records produced: 9883, total time spent: 562ms, current throughput: 16K rec/s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress before flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10761, message completed: 10760, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress before flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10985, message completed: 10984, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closing VeniceWriter for topic: wine-ratings-store_v5, gracefulness: true\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closing VeniceWriter for topic: wine-ratings-store_v5, gracefulness: true\\n'\n",
      "b'24/10/31 21:49:08 INFO KafkaProducer: [Producer clientId=wine-ratings-store_v5] Closing the Kafka producer with timeoutMillis = 497 ms.\\n'\n",
      "b'24/10/31 21:49:08 INFO KafkaProducer: [Producer clientId=wine-ratings-store_v5] Closing the Kafka producer with timeoutMillis = 497 ms.\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress before flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10980, message completed: 10906, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closed VeniceWriter for topic: wine-ratings-store_v5 in 6 ms\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closed VeniceWriter for topic: wine-ratings-store_v5 in 6 ms\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress after flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10761, message completed: 10761, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress after flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10985, message completed: 10985, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closing VeniceWriter for topic: wine-ratings-store_v5, gracefulness: true\\n'\n",
      "b'24/10/31 21:49:08 INFO KafkaProducer: [Producer clientId=wine-ratings-store_v5] Closing the Kafka producer with timeoutMillis = 499 ms.\\n'\n",
      "b'24/10/31 21:49:08 INFO VeniceWriter [8d6368e31d6e44ffb613969a1cc4ef42]: Closed VeniceWriter for topic: wine-ratings-store_v5 in 2 ms\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Kafka message progress after flushing and closing producer:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractPartitionWriter: Message sent: 10980, message completed: 10980, message errored: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 3400 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:08 INFO Executor: Finished task 2.0 in stage 1.0 (TID 4). 3400 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 3400 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1187 ms on venice-client-jupyter (executor driver) (1/3)\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 1187 ms on venice-client-jupyter (executor driver) (2/3)\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1188 ms on venice-client-jupyter (executor driver) (3/3)\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: ShuffleMapStage 1 (Venice Push Job:venice_push_job-wine-ratings-store_v5) finished in 1.211 s\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: looking for newly runnable stages\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: running: Set()\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: waiting: Set()\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: failed: Set()\\n'\n",
      "b'24/10/31 21:49:08 INFO CodeGenerator: Code generated in 5.122167 ms\\n'\n",
      "b'24/10/31 21:49:08 INFO SparkContext: Starting job: Venice Push Job:venice_push_job-wine-ratings-store_v5\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Got job 1 (Venice Push Job:venice_push_job-wine-ratings-store_v5) with 1 output partitions\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Final stage: ResultStage 4 (Venice Push Job:venice_push_job-wine-ratings-store_v5)\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Missing parents: List()\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at Venice Push Job:venice_push_job-wine-ratings-store_v5), which has no missing parents\\n'\n",
      "b'24/10/31 21:49:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.1 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.1 MiB)\\n'\n",
      "b'24/10/31 21:49:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on venice-client-jupyter:44979 (size: 5.5 KiB, free: 434.3 MiB)\\n'\n",
      "b'24/10/31 21:49:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at Venice Push Job:venice_push_job-wine-ratings-store_v5) (first 15 tasks are for partitions Vector(0))\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (venice-client-jupyter, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\\n'\n",
      "b'24/10/31 21:49:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)\\n'\n",
      "b'24/10/31 21:49:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on venice-client-jupyter:44979 in memory (size: 14.1 KiB, free: 434.4 MiB)\\n'\n",
      "b'24/10/31 21:49:08 INFO ShuffleBlockFetcherIterator: Getting 3 (180.0 B) non-empty blocks including 3 (180.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\\n'\n",
      "b'24/10/31 21:49:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\\n'\n",
      "b'24/10/31 21:49:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 2649 bytes result sent to driver\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 35 ms on venice-client-jupyter (executor driver) (1/1)\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: ResultStage 4 (Venice Push Job:venice_push_job-wine-ratings-store_v5) finished in 0.043 s\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\\n'\n",
      "b'24/10/31 21:49:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\\n'\n",
      "b'24/10/31 21:49:08 INFO DAGScheduler: Job 1 finished: Venice Push Job:venice_push_job-wine-ratings-store_v5, took 0.047291 s\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob: Accumulator values for data writer job:\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Output Records: 32726\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Empty Records: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Key Size: 1324916\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Uncompressed Value Size: 11964821\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Compressed Value Size: 11964821\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Gzip Compressed Value Size: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Total Zstd Compressed Value Size: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Spray All Partitions Triggered: 1\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Partition Writers Closed: 3\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Repush TTL Filtered Records: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   ACL Authorization Failures: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Record Too Large Failures: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Duplicate Key With Identical Value: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO AbstractDataWriterSparkJob:   Duplicate Key With Distinct Value: 0\\n'\n",
      "b'24/10/31 21:49:08 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:08 INFO VenicePushJob: Data writer job summary: \\n'\n",
      "b'\\tTotal number of records: 32726\\n'\n",
      "b'\\tSize of keys: 1.3 MiB\\n'\n",
      "b'\\tSize of uncompressed values: 11.4 MiB\\n'\n",
      "b'\\tConfigured value compression strategy: NO_OP\\n'\n",
      "b'\\tSize of compressed values: 11.4 MiB\\n'\n",
      "b'\\tFinal data size stored in Venice: 12.7 MiB\\n'\n",
      "b'\\tCompression Metrics collection: Disabled\\n'\n",
      "b'24/10/31 21:49:08 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:08 INFO VenicePushJob: \\t\\tNew overall details: Helix assignment complete\\n'\n",
      "b'24/10/31 21:49:09 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:09 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:10 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:10 INFO VenicePushJob: \\t\\tNew overall details: kicked off buffer replay\\n'\n",
      "b'24/10/31 21:49:10 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:11 INFO ControllerClient: Discovered leader controller http://venice-controller:5555 from http://venice-controller:5555\\n'\n",
      "b'24/10/31 21:49:11 INFO VenicePushJob: Successfully pushed wine-ratings-store_v5 to all the regions\\n'\n",
      "b'Venice Push Job Completed\\n'\n",
      "b'24/10/31 21:49:11 INFO SparkContext: Invoking stop() from shutdown hook\\n'\n",
      "b'24/10/31 21:49:11 INFO SparkUI: Stopped Spark web UI at http://venice-client-jupyter:4041\\n'\n",
      "b'24/10/31 21:49:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\\n'\n",
      "b'24/10/31 21:49:11 INFO MemoryStore: MemoryStore cleared\\n'\n",
      "b'24/10/31 21:49:11 INFO BlockManager: BlockManager stopped\\n'\n",
      "b'24/10/31 21:49:11 INFO BlockManagerMaster: BlockManagerMaster stopped\\n'\n",
      "b'24/10/31 21:49:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\\n'\n",
      "b'24/10/31 21:49:11 INFO SparkContext: Successfully stopped SparkContext\\n'\n",
      "b'24/10/31 21:49:11 INFO ShutdownHookManager: Shutdown hook called\\n'\n",
      "b'24/10/31 21:49:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-42f510bd-1368-4a31-9989-c057bda673cf\\n'\n",
      "b'24/10/31 21:49:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a9c131a-1476-40b0-a87e-ff10fc8afca6\\n'\n"
     ]
    }
   ],
   "source": [
    "# Path to the push job jar Java JAR file\n",
    "jar_path = \"bin/venice-push-job-all.jar\"\n",
    "\n",
    "# Arguments for spark-submit\n",
    "spark_submit_args = [\n",
    "    \"spark-submit\",\n",
    "    \"--class\", \"com.linkedin.venice.hadoop.VenicePushJob\",  # Push job main class\n",
    "    jar_path,\n",
    "    \"batch-push-job.properties\"  # Configurations\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(spark_submit_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeaaee5-85f3-4efc-87ea-b89d9c4ace25",
   "metadata": {},
   "source": [
    "### Query the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100be71-2273-4f56-b448-98325be6a948",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "And now finall, let's query this dataset from Venice!!\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d77b021f-d447-4b61-9888-4d703d8dc4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n'\n",
      "b'com/linkedin/avro/fastserde/generated/deserialization/AVRO_1_10/MultiGetResponseRecordV1_GenericDeserializer_433464781_433464781.java\\n'\n",
      "b'Key string parsed successfully. About to make the query.\\n'\n",
      "b'com/linkedin/avro/fastserde/generated/deserialization/AVRO_1_10/value_GenericDeserializer_1319288651_1319288651.java\\n'\n",
      "b'key-class=java.lang.String\\n'\n",
      "b'value-class=org.apache.avro.generic.GenericData.Record\\n'\n",
      "b'request-path=storage/wine-ratings-store/OkEuIE1hbm8gUHVnbGlhIFByaW1pdGl2byAyMDA2?f=b64\\n'\n",
      "b'key=A. Mano Puglia Primitivo 2006\\n'\n",
      "b'value={\"region\": \"Italy\", \"variety\": \"Red Wine\", \"rating\": 88.0, \"notes\": \"Made from 100% Primitivo (Zinfandel\\'s ancestor) from Puglia. A. Mano (meaning \\\\\"made by hand\\\\\") is made from 70-100 year-old, hand-cultivated, low-yielding vines. \"}\\n'\n"
     ]
    }
   ],
   "source": [
    "# Query the store for the data\n",
    "query_store_args = [\n",
    "    \"./fetch.sh\",\n",
    "    \"http://venice-router:7777\", \n",
    "    \"wine-ratings-store\",\n",
    "    \"A. Mano Puglia Primitivo 2006\",\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(query_store_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb39a7-66fe-43cc-944a-d0ac9aa235e5",
   "metadata": {},
   "source": [
    "__Saluti!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c214620-c1a5-42c9-bf3c-c1ecad0b4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to clean up your spark session afterward\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
